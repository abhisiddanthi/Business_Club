<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="og:image" content="http://bclub.co.in/blog/img/bank.jpg"/>
    <meta property="fb:admins" content="100006371020407"/>
    <meta property="fb:admins" content="100004330893852"/>
<meta name="og:title" content="World bank and its failure" />
<meta name="og:description" content="Have Global Governance Institutions Failed Developing Countries?" />
<meta name="og:url" content="http://bclub.co.in/blog/deep_fake.html" />
<meta property="og:image" content="http://bclub.co.in/blog/img/bank.jpg" />
    <title>REGRESSION MODELS</title>
    <link rel="shortcut icon" href="img/favicon.ico">
<!-- Google analytics code -->
<script>
  (function(i, s, o, g, r, a, m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)}, i[r].l=1*new Date();a=s.createElement(o), 
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-65026905-2', 'auto');
  ga('send', 'pageview');

</script>
<!-- Google analytics end-->
    <!-- Bootstrap Core CSS -->
    <link href="css\bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css\clean-blog.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Lora:400, 700, 400italic, 700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic, 400italic, 600italic, 700italic, 800italic, 400, 300, 600, 700, 800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

<style type="text/css">

    #manual td, th{
         padding: 5px;

     }
     #manual td:first-of-type {
        font-weight: bold;
     }

    div.color{
       height:15px;
       width: 15px;
       float: left;
       margin-right: 5px;
    }

    #reference .color{
       height:13px;
       width: 13px;        
    }

    div#moderate{
        background-color: green;

    }
        
    div#significant{
        background-color: yellow;
    }    

    div#severe{
        background-color: red;
    }

    #manual td, th{
        border: 2px solid black;
    }

    #empty{
        border: none;
    }

</style>

</head>

<body>
<!-- Facebook SDK added on 10/7/15 -->
<script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '836980526380578', 
      xfbml      : true, 
      version    : 'v2.4'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/en_US/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>
<div id="fb-root"></div> <!--Added on 17/08 for like/share -->
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.4&appId=836980526380578";
  fjs.parentNode.insertBefore(js, fjs);F
}(document, 'script', 'facebook-jssdk'));</script><!-- Facebook SDK ends -->


    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top" style="color:black;">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span c lass="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="../../index.html" style="margin-top:-5px;color:white;"><img src="images/clublogo.png" style="display:inline; width:36px"> Business Club</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="navbar-nav  navbar-right nav">
                    <li>
                        <a href="../../blog/index.html">Blogs</a>
                    </li>
                    <li>
                        <a href="../../modules.html">Modules</a>
                    </li>
                    <li>
                        <a href="../../contact/contact.html">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>


    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="margin:0;padding:0;background-image: url('images/regression1.png');background-size:fill;">
    <div class="post-heading" align="center" style="padding-top:199px;padding-bottom:50px;">
                        <h1 style="margin-top:125px;color:white;">REGRESSION MODELS</h1>
                        <span class="meta" >Posted by <a href="#">B-Club, IIT Kharagpur</a> on October 4, 2017</span>
                    </div>
    
    </header>

    <!-- Post Content -->
    <article>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1" style="text-align: justify;  text-justify: inter-word;">
                    <h2>What is Linear Regression?</h2>
                    <p>Simply put, we predict scores on one variable from the scores on a second variable</p>
                    <p>The variable we are predicting is called the <b>criterion variable and is referred to as Y.</b>
                        The variable we are basing our predictions on is called the <b>predictor variable and is
                            referred to as X</b>. When there is only one predictor variable, the prediction method is
                        called simple regression.</p>
                    <p>In simple linear regression, the predictions of Y when plotted as a function of X form a
                        straight line. A simple regression line looks something like the figure shown below</p>
                    <p style="text-align: center;"><img src="images/regression2.png" width="100%"/></p>

                    <p><b><i>How to find this line?</i></b></p>

                    <p>YBy far, the most commonly-used criterion for the best-fitting line is the line that
                        minimizes the sum of the squared errors of prediction. That is the criterion that was
                        used to find the regression line. The sum of the squared errors of prediction is lower
                        than it would be for any other regression line. We’ll look into the mathematics behind
                        minimizing this error would be discussed later
                        .</p>

                    <h2>Motivation</h2>
                    
                    <p style="color: red;">Before we get into the tricky mathematical equations, we’ll look into the question “Where do I use regression?”</p>
                    
                    <p style="text-align: center;"><img src="images/regression3.png" width="100%"/></p>
                    
                    <p>There are 14 attributes in each case of the dataset. They are:</p>

                    <ol>
                        <li>CRIM - per capita crime rate by town</li>
                        <li>ZN - proportion of residential land zoned for lots over 25,000 sq.ft.</li>
                        <li>INDUS - proportion of non-retail business acres per town.
                        </li>
                        <li>CHAS - Charles River dummy variable (1 if tract bounds river; 0
                            otherwise)</li>
                        <li>NOX - nitric oxides concentration (parts per 10 million)</li>
                        <li>RM - average number of rooms per dwelling
                        </li>
                        <li>AGE - proportion of owner-occupied units built prior to 1940
                        </li>
                        <li>DIS - weighted distances to five Boston employment centres</li>
                        <li>RAD - index of accessibility to radial highways</li>
                        <li>.TAX - full-value property-tax rate per $10,000</li>
                        <li>.PTRATIO - pupil-teacher ratio by town</li>
                        <li>B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by
                            town</li>
                            <li>.LSTAT - % lower status of the population</li>
                            <li><b>.MEDV - Median value of owner-occupied homes in $1000's</b></li>
                    </ol>
                    <center><b>MEDV - Median value of owner-occupied homes in $1000's</b></center>
                    
                    <p>For cases like these when value of one variable, (here, the value of homes) depends on
                        various factors, we retort to linear regression. Before actually performing regression it is
                        essential to perform some data exploration steps, since often variables tend to be highly
                        correlated.</p>
                    
                    <h2>Mathematical Insight</h2>
                    
                    <p> The calculations are based on the statistics MX is the mean of X, MY is the mean of Y, sX is the standard deviation of X, sY is the
                        standard deviation of Y, and r is the correlation between X and Y.
                        The slope (b) can be calculated as follows:<br>
                        b = r sY
                        /sX
                        <br>and the intercept (A) can be calculated as:<br>
                        A = MY
                         - bMX
                        .
                        </p>

                    <p>Correlation constant
                    </p>

                    <p style="text-align: center;"><img src="images/regression4.png" width="100%"/></p>
                    <p>where the "sigma" symbol indicates summation and n stands for the number of data
                        points. With these quantities computed, the correlation coefficient is defined as:</p>
                    <p style="text-align: center;"><img src="images/regression5.png" width="100%"/></p>
                    <p><b>Hypothesis Function :</b> The hypothesis function for linear regression is given by: </p>
                    <p style="text-align: center;"><img src="images/regression6.png" width="100%"/></p>
                    <p>Where, n : number of features<br>
                        θj = parameter associated with j th feature</p>

                    <p><b>Cost function :</b> We define a function, known as the cost function with which we can
                        relate the error of our model. So a lower the cost function would imply a better model.<br>
                        In general, the cost function gives us the cost for producing a particular output
                        The cost function is what is minimised to obtain the best fitting curve.<br>
                        For linear regression the cost function is given by :</p>
                        <p style="text-align: center;"><img src="images/regression7.png" width="100%"/></p>

                    <h2>Gradient Descent:</h2>
                    <p>Gradient descent is an optimization algorithm used to find the values of parameters
                        (coefficients) of a function (f) that minimizes a cost function .</p>
                        <p style="text-align: center;"><img src="images/regression8.png" width="100%"/></p>
                    <p>For a regression with two parameters θo and θ1 , the ‘cost space ‘ plot looks like this.
                        The aim of the gradient descent algorithm is to find such values of θ0 and θ1 so that
                        the cost J(θ) is minimized .</p>

                    <h3>The Algorithm</h3>

                    <p>There is a fixed algorithm by which we vary θ0 and θ1 which is given below :</p>
                    <p style="text-align: center;"><img src="images/regression9.png" width="100%"/></p>
                    <p> If is &alpha; too small, a large number of iterations is required before cost function
                        converges </p>

                    <p> Now, if &alpha; is too large, then there is a chance that it may fail to converge or it can even
                        diverge.
                        </p>

                    <p>How will one really know that he has already reached the minimum? It turns out that at
                        the local minimum, derivative will be equal to zero because the slope of the tangent line
                        at this point will be equal to zero. So, if the parameters are already at a local minimum
                        then one step with gradient descent does absolutely nothing and that is what we are
                        looking for</p>

                    <p style="text-align: center;"><img src="images/regression10.png" width="100%"/></p>

                    <p>For multivariate linear regression, the same algorithm for gradient descent will be
                        followed. The only difference would be that we j takes values from 0 to n if there are n
                        features (excluding the constant term)</p>
                        <p style="text-align: center;"><img src="images/regression11.png" width="100%"/></p>
                    <p>This is a contour plot of the cost function with θ0 and θ1 on x-axis and y-axis
                        respectively.As we move closer towards the centre of this plot, we get those values of
                        θ0 and θ1 which give the minimum value of the cost function.
                        </p>

                    <h3>Model Validation</h3>
                    <p style="text-align: center;"><img src="images/regression12.png" width="100%"/></p>
                    
                    <p><b>Under-fitting -</b><br>
                        Underfitting refers to a model that can neither model the training data nor generalize to
                        new data.
                        An underfit machine learning model is not a suitable model and will be obvious as it will
                        have poor performance on the training data.
                        </p>

                    <p><b>Over-fitting -</b><br>
                        Overfitting refers to a model that models the training data too well.
                        Overfitting happens when a model learns the detail and noise in the training data to the
                        extent that it negatively impacts the performance of the model on new data. This means
                        that the noise or random fluctuations in the training data is picked up and learned as
                        concepts by the model. The problem is that these concepts do not apply to new data
                        and negatively impact the models ability to generalize.
                        
                                                </p>
                    <p><b>Cross Validation</b><br>To limit overfitting and underfitting we use cross - validation. Cross validation is of 3
                        types :</p>

                    <p><b>1.Hold - out method</b><br>
                        Here we randomly divide the available set of samples into two parts: a training set and a
                        validation or hold-out set.<br>
                        The model is fit on the training set, and the fitted model is used to predict the responses
                        for the observations in the validation set.<br>
                        The resulting validation-set error provides an estimate of the test error.</p>

                    <p><b>Limitations -</b>
                        The validation estimate of the test error can be highly variable, depending on precisely
                        which observations are included in the training set and which observations are included
                        in the validation set.<br>
                        In the validation approach, only a subset of the observations — those that are included
                        in the training set rather than in the validation set — are used to fit the model.<br>
                        This suggests that the validation set error may tend to overestimate the test error for the
                        model fit on the entire data set.
                    </p>

                    <p><b>2.K-fold Cross Validation</b>
                        Estimates can be used to select best model, and to give an idea of the test error of the
final chosen model.<br>
Randomly divide the data into K equal-sized parts. We leave out part k, fit the model to
the other K−1 parts (combined), and then obtain predictions for the left-out kth part.
This is done in turn for each part k = 1, 2,...K , and then the results are combined.
Setting K = n yields n-fold or leave-one out Cross-validation (LOOCV).

                    </p>

                    <h3>Tips and Tricks:</h3>

                    <p><b>Normalization:</b></p>

                    <p>The regression equation is simpler if variables are standardized so that their means are
                        equal to 0 and standard deviations are equal to 1 [N(0,1)].
                        </p>

                    <p>This makes the regression line:<br>
                        <center>ZY'
                            = (r)(ZX
                           )</center>
                        where ZY'
                         is the predicted standard score for Y,<br>
                        r is the correlation,<br>
                        ZX
                         is the standardized score for X.<br></p>
                        <p>Note that the slope of the regression equation for standardized variables is r</p>

                    <p><b>Regularization:</b><br>
                    Regularization is technique used to avoid over fitting.
                    If we are over fitting model then, it requires penalizing theta parameters in order to
                    make just right fit. This will lead to use regularization in model fitting.<br>
                    Now cost function will be defined as below :</p>
                    <p style="text-align: center;"><img src="images/regression13.png" width="100%"/></p>

                    <p>Hence gradient descent for regularised linear regression will be:</p>

                    <p style="text-align: center;"><img src="images/regression14.png" width="100%"/></p>

                    <h2>Quantifying Parameters:
                    </h2>
                    
                    <h3>R squared:</h3>
                    <p>
                        R-squared , known as the coefficient of determination is a statistical measure of how
                        close the data are to the fitted regression line.<br>
                        Calculation of R squared:</p>

                    <p>Since R2
                        is a proportion, it is always a number between 0 and 1.<br>
                       If R 2
                        = 1, all of the data points fall perfectly on the regression line.<br>
                       If R2
                        = 0, the estimated regression line is perfectly horizontal.</p>

                    <h3>Problems with R-squared</h3>

                    <p>Every time a predictor is added to a model, the R-squared increases, even if due to
                        chance alone. It never decreases. Consequently, a model with more terms may appear
                        to have a better fit simply because it has more terms.</p>

                    <h3>Adjusted R -squared</h3>

                    <p>The adjusted R-squared compares the explanatory power of regression models that
                        contain different numbers of predictors .
                        The adjusted R-squared increases only if the new term improves the model more than
                        would be expected by chance. It decreases when a predictor improves the model by
                        less than expected by chance</p>
                    
                    <h3>P-value:</h3>

                    <p>The P-value is the probability that our data would be at least this inconsistent with the
                        hypothesis, assuming the hypothesis is true.
                        The p-value for each independent variable tests the null hypothesis - that the
                        variable has no correlation with the dependent variable. If there is no correlation,
                        there is no association between the changes in the independent variable and the shifts
                        in the dependent variable. In other words, there is no effect.
                        If the p-value for a variable is less than your significance level, your sample data
                        provide enough evidence to reject the null hypothesis for the entire population. Your
                        data favors the hypothesis that there is a non-zero correlation. Changes in the
                        independent variable are associated with changes in the response at the population
                        level. This variable is statistically significant and probably a worthwhile addition to your
                        regression model.<br>
                        On the other hand, a p-value that is greater than the significance level indicates that
there is insufficient evidence in your sample to conclude that a non-zero correlation
exists.<br>
For example, let’s say we wanted to know if a new drug had an influence on IQ. These
are what we would want to pick as our null and alternative hypotheses:<br>
<b><i> - Null hypothesis –</i></b> The average IQ of a population that uses the drug will be the same
as the average IQ of a population that does not use the drug.<br>
<b><i> - Alternative hypothesis – </i></b>The average IQ of a population that uses the drug will be
different from the average IQ of a population that does not use the drug.
These are the only two options, so if we reject the null hypothesis, we can accept the
alternative hypothesis
                    </p>

                    <p>In order to reject the null hypothesis, we need to pick a level of statistical significance.
                        By default, this is 5 or 1 percent. If we get a P-value smaller than our significance level,
                        we can reject the null hypothesis.<br>
                        In other words, a predictor that has a low p-value is likely to be a meaningful addition to
                        your model because changes in the predictor's value are related to changes in the
                        response variable.
                        </p>

                        <h3>T-value:</h3>
                        <p>The "t'' statistic is computed by dividing the estimated value of the parameter by its
                            standard error.<br>
                            (The standard error is an estimate of the standard deviation of the coefficient, the
                            amount it varies across cases).This statistic is a measure of the likelihood that the
                            actual value of the parameter is not zero. The larger the absolute value of t, the less
                            likely that the actual value of the parameter could be zero.<br>
                        <center>t = Coeff / SE</center>
                    </p>
                    <h3>Interpreting the model :</h3>
                        <p>Using all the features given in the dataset , the model created showed the following
                        summary</p>

                    <p style="text-align: center;"><img src="images/regression16.png" width="100%"/></p>
					<p>Notice that the value P value for two features INDUS and AGE is much high , indicating
                        the variables are not significant to be kept in the model .<br>
                        Removing the two features would not affect the model fit that much .</p>

                    <h3>Correlation Matrix:</h3>
                    <p style="text-align: center;"><img src="images/regression17.png" width="100%"/></p>

                    <p>From the matrix we observe that RAD and TAX have a high correlation of 0.91. So we
                        first remove RAD and check if the model has improved. Then we do the same with TAX
                        and then take the better model out of the two.</p>
                    <!-- sample comment -->
                    <div class="fb-comments" data-href="http://bclub.co.in/blog/deep_fake.html" data-numposts="5" width=750></div>
                    <!-- sample comment ends -->
                    <div class="fb-like" data-href="http://bclub.co.in/blog/deep_fake.html" data-layout="standard" data-action="like" data-show-faces="false" data-share="true" width="100%"></div>
                </div>
            </div>
        </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.facebook.com/bclubkgp" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <p class="copyright text-muted"><img src="img/clublogo.jpg">  Copyright &copy; Business Club, IIT Kharagpur</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>
    <img alt="Have Global Governance Institutions Failed Developing Countries?" rel="Facebook image" src="http://bclub.co.in/blog/img/bank.jpg" style="display: none;">  
</body>

</html>
